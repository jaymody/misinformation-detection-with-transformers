"""Preprocessors."""
import nltk
import heapq
import logging

import numpy as np
from scipy import spatial
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

from . import utils
from . import modeling

_logger = logging.getLogger(__name__)


class ClaimPreprocessor:
    """Preprocessor for Claims."""

    def __init__(self, articles, word2vec_path):
        """Constructor for ClaimPreprocessor.

        Parameters
        ----------
        articles : dict of (id, Article)
            Dict of id to Article.
        word2vec_path : str
            Path to saved word2vec model file.
        """
        self.articles = articles
        self.word2vec_model = utils.load_word2vec(word2vec_path)

    def generate_examples(self, claim, keep_n=8, min_threshold=0.40, min_examples=4, update_support=True):
        """Generates `InputExample`s for the claim using related articles.

        Examples are generated by searching the claims related articles for
        sentences that best match the claim. These sentences are then used to
        as the `text_b` portion of the InputExample (while `text_a` is taken
        from the claim itself). The relationship between a claim and a sentence
        is quanitified by a score that is computed using the following steps:

        1.  Compute the tfidf and word2vec embeddings for the claim and all
            sentences in the related articles.
        2.  Take the cosine similarity between each of claim-sentence pair
            (one tfidf score and one word2vec score for each pair).
        3.  Take the average of the tfidf and word2vec similarity scores

        Parameters
        ----------
        claim : str
            A Claim.
        keep_n : int
            Max number of related sentences to keep.
        min_threshold : float
            Minimum score (between 0 and 1) the claim-sentence pair must be
            achieved to be included in the output (after `min_examples` examples
            have been chosen).
        min_examples : int
            Minimum number of examples to include in the output.
        update_support : bool
            Whether to update the support field of the claim with the generated
            scores and sentences.

        Returns
        -------
        examples : list
            Returns a list of the generated InputExamples.
        """
        # get sentences from related_articles
        corpus = [(ref,sentence) for ref in claim.related_articles for sentence in self.articles[str(ref)].body]
        references, sentences = map(list, zip(*corpus))

        # append and pad claim sentence
        sentences.append(claim)
        references.append(None)

        # get tf_idf vectors
        _logger.info("... generating tdidf vectors ...")
        tfidf_vectorizer = TfidfVectorizer()
        tfidf_vectors = tfidf_vectorizer.fit_transform(sentences)

        # get sentence word2vec vectors
        _logger.info("... fetching word2vec embeddings ...")
        def word2vec_sentence(sentence):
            words = nltk.tokenize.word_tokenize(sentence)
            vectors = []
            for word in words:
                try:
                    vectors.append(self.word2vec_model[word])
                except:
                    continue
            return np.nan if not vectors else np.mean(vectors, axis=0)
        word2vec_vectors = [word2vec_sentence(sentence) for sentence in sentences]

        # calculate and sort cosine similarities for embeddings
        _logger.info("... computing cosine similarity scores ...")
        support = []
        for ref, sentence, tfidf_vector, word2vec_vector in zip(references, sentences, tfidf_vectors, word2vec_vectors):
            tfidf_score = float(cosine_similarity(tfidf_vectors[-1], tfidf_vector))
            word2vec_score = 0.0 if np.isnan(np.min(word2vec_vector)) else float(1 - spatial.distance.cosine(word2vec_vectors[-1], word2vec_vector))
            support.append({
                "source_article": ref,
                "text": sentence,
                "scores": {
                    "tfidf": tfidf_score,
                    "word2vec": word2vec_score
                },
                "score": float(sum(tfidf_score, word2vec_score)) / 2
            })
        support.pop() # remove the claim sentence itself from support

        # sort and get nlargest
        if keep_n:
            support = heapq.nlargest(keep_n, support, key=lambda x: x["score"])
            support = sorted(support, key=lambda x: x["score"], reverse=True)
            support = [s for i,s in enumerate(support) if s["score"] >= min_threshold or i < min_examples]
        else:
            support = sorted(support, key=lambda x: x["score"], reverse=True)

        examples = []
        for s in support:
            examples.append(modeling.InputExample(
                guid=claim.id,
                text_a=claim.claim,
                text_b=s["text"],
                label=claim.label
            ))

        if update_support:
            claim.support = support

        return examples
