{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test BasicDataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import InputExample, BertTokenizer\n",
    "from valerie.datasets import BasicDataset\n",
    "from valerie.utils import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101419/101419 [00:00<00:00, 458262.45it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/phase1/all_examples.json\") as fi:\n",
    "    examples = [InputExample(**e) for e in tqdm(json.load(fi))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101419"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-27 13:48:17,961] INFO:transformers.tokenization_utils: loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/jay/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-27 13:48:19,568] INFO:valerie.datasets: ... converting examples to features ...\n",
      "100%|██████████| 101419/101419 [02:56<00:00, 575.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176.5986201763153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# single cpu\n",
    "start = time.time()\n",
    "dataset = BasicDataset(examples, tokenizer, label_list=[0,1,2])\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-27 13:51:16,182] INFO:valerie.datasets: ... converting examples to features ...\n",
      "100%|██████████| 101419/101419 [00:59<00:00, 1691.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.12821292877197\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing\n",
    "start = time.time()\n",
    "dataset = BasicDataset(examples, tokenizer, label_list=[0,1,2], nproc=6)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101419/101419 [02:56<00:00, 574.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "assert len(examples) == len(dataset.features)\n",
    "for i in tqdm(range(len(examples))):\n",
    "    assert dataset.features[i].input_ids == tokenizer.encode_plus(examples[i].text_a, examples[i].text_b, max_length=tokenizer.max_len, pad_to_max_length=True).input_ids, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-27 13:58:24,121] INFO:valerie.datasets: .. saving features to cached file logs/temp.cache\n"
     ]
    }
   ],
   "source": [
    "dataset.save(\"logs/temp.cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310M\tlogs/temp.cache\n"
     ]
    }
   ],
   "source": [
    "!du -sh logs/temp.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-04-27 14:05:41,256] INFO:valerie.datasets: ... loading features from cached file logs/temp.cache\n"
     ]
    }
   ],
   "source": [
    "from_cache_dataset = BasicDataset(None, tokenizer, label_list=[0,1,2], cached_features_file=\"logs/temp.cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# heapq.nlargest() result is Sorted\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import heapq\n",
    "import timeit\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(l, n=5):\n",
    "    return [l.pop(np.argmax(l)) for _ in range(n)]\n",
    "    \n",
    "def partitioner(l, n=5):\n",
    "    return [l[i] for i in np.argpartition(l, -n)[-n:][::-1]]\n",
    "    \n",
    "def heaper(l, n=5):\n",
    "    return heapq.nlargest(n, l)\n",
    "    \n",
    "def sorter(l, n=5):\n",
    "    return sorted(l, reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(np.random.rand(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert iterator(copy.copy(l)) ==  partitioner(copy.copy(l)) == heaper(copy.copy(l)) == sorter(copy.copy(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = \"import numpy as np; import heapq; l = list(np.random.rand(60)); n=5\"\n",
    "number = 1\n",
    "\n",
    "print(timeit.timeit(\"[l.pop(np.argmax(l)) for _ in range(n)]\", setup=setup, number=number))\n",
    "print(timeit.timeit(\"[l[i] for i in np.argpartition(l, -n)[-n:][::-1]]\", setup=setup, number=number))\n",
    "print(timeit.timeit(\"heapq.nlargest(n, l)\", setup=setup, number=number))\n",
    "print(timeit.timeit(\"sorted(l, reverse=True)[:n]\", setup=setup, number=number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scipy cosine vs sklearn cosine\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "setup1 = \"import numpy as np; arrs1 = [np.random.rand(400) for _ in range(60)];arrs2 = [np.random.rand(400) for _ in range(60)]\"\n",
    "setup2 = \"import numpy as np; arrs1 = [np.random.rand(400) for _ in range(60)];arrs2 = [np.random.rand(400) for _ in range(60)]\"\n",
    "\n",
    "import1 = \"from sklearn.metrics.pairwise import cosine_similarity\"\n",
    "stmt1 = \"[float(cosine_similarity(arr1.reshape(1,-1), arr2.reshape(1,-1))) for arr1, arr2 in zip(arrs1, arrs2)]\"\n",
    "\n",
    "import2 = \"from scipy.spatial.distance import cosine\"\n",
    "stmt2 = \"[float(1 - cosine(arr1, arr2)) for arr1, arr2 in zip(arrs1, arrs2)]\"\n",
    "\n",
    "print(\"sklearn: \", timeit.timeit(stmt1, setup=import1 + \";\" + setup1, number=1000))\n",
    "print(\"scipy:   \", timeit.timeit(stmt2, setup=import2 + \";\" + setup2, number=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
