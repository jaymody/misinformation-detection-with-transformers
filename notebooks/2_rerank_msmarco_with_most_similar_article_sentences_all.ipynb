{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFcE0lSqYag4"
   },
   "source": [
    "# Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruKsHPGPblRV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import heapq\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from valerie.utils import get_logger\n",
    "from valerie.preprocessing import extract_words_from_url, clean_text\n",
    "from valerie.scoring import validate_predictions_phase2, compute_score_phase2\n",
    "from valerie.modeling import SequenceClassificationModel, SequenceClassificationDataset, SequenceClassificationExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sT0p85DUbe5L"
   },
   "outputs": [],
   "source": [
    "_logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTPKlXP4dElX"
   },
   "outputs": [],
   "source": [
    "with open(\"data/phase2-3/processed/responses.pkl\", \"rb\") as fi:\n",
    "    responses = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1E-o-DMbBsR"
   },
   "outputs": [],
   "source": [
    "def compute_responses_score(results, claims_dict):\n",
    "    predictions = {}\n",
    "    perfect_predictions = {}\n",
    "    labels = {}\n",
    "\n",
    "    for k, hits in results.items():\n",
    "        claim = claims_dict[k]\n",
    "        labels[claim.id] = claim.to_dict()\n",
    "        \n",
    "        hits = sorted(hits, key=lambda x: x[1], reverse=True) # sort by score\n",
    "        predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits[:2]])\n",
    "            }\n",
    "        }\n",
    "        perfect_predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits if v[0] in claim.related_articles.values()][:2])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    validate_predictions_phase2(predictions)\n",
    "    score = compute_score_phase2(labels, predictions)\n",
    "    validate_predictions_phase2(perfect_predictions)\n",
    "    perfect_score = compute_score_phase2(labels, perfect_predictions)\n",
    "    return {\n",
    "        \"perfect_rerank_score\": perfect_score[\"score\"],\n",
    "        \"perfect_rerank_error\": perfect_score[\"error\"],\n",
    "        \"api_score\": score[\"score\"],\n",
    "        \"api_error\": score[\"error\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_a(claim):\n",
    "    text_a = claim.claim\n",
    "    text_a += \" \"\n",
    "    text_a += claim.claimant if claim.claimant else \"no claimant\"\n",
    "    text_a += \" \"\n",
    "    text_a += claim.date.split()[0].split(\"T\")[0] if claim.date else \"no date\"\n",
    "    return clean_text(text_a)\n",
    "\n",
    "def create_text_b_content(article):\n",
    "    text_b = \"\"\n",
    "    if article.source:\n",
    "        text_b += article.source + \". \"\n",
    "    if article.title:\n",
    "        text_b += article.title + \". \"\n",
    "    if article.url:\n",
    "        url_words = extract_words_from_url(article.url)\n",
    "        if url_words:\n",
    "            text_b += \" \".join(url_words) + \". \"\n",
    "    if article.content:\n",
    "        text_b += article.content\n",
    "    return clean_text(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spacy on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses = 0\n",
    "claims_list = []\n",
    "for res in tqdm(responses):\n",
    "    if not res[\"res\"]:\n",
    "        misses += 1\n",
    "        continue\n",
    "    claim = res[\"claim\"]\n",
    "    claim.text_a = create_text_a(claim)\n",
    "    claim.res = res\n",
    "    claim.support = {}\n",
    "    claims_list.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_texts = [claim.text_a for claim in claims_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_docs = [doc for doc in tqdm(nlp.pipe(claims_texts, n_process=16, disable=[\"textcat\", \"tagger\", \"parser\", \"ner\"]), total=len(claims_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dict = {}\n",
    "for claim, doc in tqdm(zip(claims_list, claims_docs)):\n",
    "    claim.doc = doc\n",
    "    claims_dict[claim.index] = claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misses = 0\n",
    "articles_list = []\n",
    "for res in tqdm(responses):\n",
    "    if not res[\"res\"]:\n",
    "        misses += 1\n",
    "        continue\n",
    "    for hit in res[\"res\"][\"hits\"][\"hits\"]:\n",
    "        article = hit[\"article\"]\n",
    "#         article.text_b = create_text_b_content(article)\n",
    "        articles_list.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = list(set(articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _text_b_text(article):\n",
    "    return article, create_text_b_content(article)\n",
    "\n",
    "articles_texts = {}\n",
    "pool = multiprocessing.Pool(16)\n",
    "for article, text_b in tqdm(pool.imap_unordered(_text_b_text, articles_list), total=len(articles_list)):\n",
    "    articles_texts[article.index] = text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles_list:\n",
    "    article.text_b = articles_texts[article.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_texts = [article.text_b for article in tqdm(articles_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_docs = [doc for doc in tqdm(nlp.pipe(articles_texts, n_process=16, disable=[\"textcat\", \"tagger\", \"ner\"]), total=len(articles_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dict = {}\n",
    "for article, doc in tqdm(zip(articles_list, articles_docs)):\n",
    "    article.doc = doc\n",
    "    articles_dict[article.index] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYiga7bObTzu"
   },
   "source": [
    "# Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_b_curated(article, claim):\n",
    "    support = []\n",
    "    for sent in article.doc.sents:\n",
    "        support.append({\n",
    "            \"text\": sent.text,\n",
    "            \"score\": claim.doc.similarity(sent)\n",
    "        })\n",
    "    support = heapq.nlargest(32, support, key=lambda x: x[\"score\"])\n",
    "    claim.support[article.index] = support\n",
    "    text_b = clean_text(\" \".join([s[\"text\"] for s in support]))\n",
    "    return text_b\n",
    "\n",
    "def gen_examples(claim):\n",
    "    hits_indices = [hit[\"url\"] for hit in claim.res[\"res\"][\"hits\"][\"hits\"]]\n",
    "    hits = [articles_dict[idx] for idx in hits_indices]\n",
    "    \n",
    "    related_articles_url_set = set(claim.related_articles.values())\n",
    "\n",
    "    examples_to_add = []\n",
    "    for article in hits:\n",
    "        article.text_b = create_text_b_curated(article, claim)\n",
    "\n",
    "        examples_to_add.append(SequenceClassificationExample(\n",
    "            guid=claim.index,\n",
    "            text_a=claim.text_a,\n",
    "            text_b=article.text_b,\n",
    "            label=1 if article.url in related_articles_url_set else 0,\n",
    "            art_id=article.index\n",
    "        ))\n",
    "    return examples_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = []\n",
    "# for examples_to_add in tqdm(pool.imap_unordered(gen_examples, claims_dict.values()), total=len(claims_dict)):\n",
    "#     examples.extend(examples_to_add)\n",
    "\n",
    "examples = []\n",
    "for claim in tqdm(claims_dict.values()):\n",
    "    hits_indices = [hit[\"url\"] for hit in claim.res[\"res\"][\"hits\"][\"hits\"]]\n",
    "    hits = [articles_dict[idx] for idx in hits_indices]\n",
    "\n",
    "    related_articles_url_set = set(claim.related_articles.values())\n",
    "\n",
    "    for article in hits:\n",
    "        article.text_b = create_text_b_curated(article, claim)\n",
    "\n",
    "        examples.append(SequenceClassificationExample(\n",
    "            guid=claim.index,\n",
    "            text_a=claim.text_a,\n",
    "            text_b=article.text_b,\n",
    "            label=1 if article.url in related_articles_url_set else 0,\n",
    "            art_id=article.index\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claims_dict))\n",
    "print(len(articles_dict))\n",
    "print()\n",
    "print(len(claims_dict)*30)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DamT2JLogphk"
   },
   "outputs": [],
   "source": [
    "print(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(claims_dict.values())[0].claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(json.dumps(list(claims_dict.values())[0].support, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCteL6dXbX53"
   },
   "outputs": [],
   "source": [
    "print(len(responses)*16)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ocq58LFblA_"
   },
   "source": [
    "# Predict\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNS6evMZbpsU"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"castorini/monot5-base-msmarco\"\n",
    "# \"castorini/monobert-large-msmarco\"\n",
    "# \"nboost/pt-bert-large-msmarco\"]:\n",
    "pretrained_model_name_or_path = \"castorini/monobert-large-msmarco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceClassificationModel.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_dataset = model.create_dataset(examples, nproc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output = model.predict(examples_dataset, predict_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2YvlHrobmdq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.918748461309641,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.48984172731830333,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "trans\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.918748461309641,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.5750774971552669,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "both\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.918748461309641,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.5405780150774728,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "claims_dict = {res[\"claim\"].index: res[\"claim\"] for res in responses if res[\"res\"]}\n",
    "api_scores_dict = {\n",
    "    res[\"claim\"].index: {\n",
    "        hit[\"article\"].index: hit[\"score\"] for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    }\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "\n",
    "rerank_just_api_responses = {\n",
    "    res[\"claim\"].index: [\n",
    "        (hit[\"article\"].index, hit[\"score\"]) for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    ]\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "rerank_just_trans_responses = {res[\"claim\"].index: [] for res in responses if res[\"res\"]}\n",
    "\n",
    "rerank_both_responses = {\n",
    "    res[\"claim\"].index: []\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "for example, proba in tqdm(zip(examples, predict_output.predictions)):\n",
    "    proba = float(proba[1]) # get probability that the article is related\n",
    "\n",
    "    rerank_just_trans_responses[example.guid].append((example.art_id, proba))\n",
    "    rerank_both_responses[example.guid].append((example.art_id, proba + api_scores_dict[example.guid][example.art_id]))\n",
    "    \n",
    "print('api')\n",
    "print(json.dumps(compute_responses_score(rerank_just_api_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('trans')\n",
    "print(json.dumps(compute_responses_score(rerank_just_trans_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('both')\n",
    "print(json.dumps(compute_responses_score(rerank_both_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "__experiment.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
