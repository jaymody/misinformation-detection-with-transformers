{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFcE0lSqYag4"
   },
   "source": [
    "# Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruKsHPGPblRV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import heapq\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from valerie.utils import get_logger\n",
    "from valerie.preprocessing import extract_words_from_url, clean_text\n",
    "from valerie.scoring import validate_predictions_phase2, compute_score_phase2\n",
    "from valerie.modeling import SequenceClassificationModel, SequenceClassificationDataset, SequenceClassificationExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nproc=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sT0p85DUbe5L"
   },
   "outputs": [],
   "source": [
    "_logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTPKlXP4dElX"
   },
   "outputs": [],
   "source": [
    "with open(\"data/phase2-validation/processed/responses.pkl\", \"rb\") as fi:\n",
    "    responses = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses = responses[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1E-o-DMbBsR"
   },
   "outputs": [],
   "source": [
    "def compute_responses_score(results, claims_dict):\n",
    "    predictions = {}\n",
    "    perfect_predictions = {}\n",
    "    labels = {}\n",
    "\n",
    "    for k, hits in results.items():\n",
    "        claim = claims_dict[k]\n",
    "        labels[claim.id] = claim.to_dict()\n",
    "        \n",
    "        hits = sorted(hits, key=lambda x: x[1], reverse=True) # sort by score\n",
    "        predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits[:2]])\n",
    "            }\n",
    "        }\n",
    "        perfect_predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits if v[0] in claim.related_articles.values()][:2])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    validate_predictions_phase2(predictions)\n",
    "    score = compute_score_phase2(labels, predictions)\n",
    "    validate_predictions_phase2(perfect_predictions)\n",
    "    perfect_score = compute_score_phase2(labels, perfect_predictions)\n",
    "    return {\n",
    "        \"perfect_rerank_score\": perfect_score[\"score\"],\n",
    "        \"perfect_rerank_error\": perfect_score[\"error\"],\n",
    "        \"api_score\": score[\"score\"],\n",
    "        \"api_error\": score[\"error\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_a(claim):\n",
    "    text_a = claim.claim\n",
    "    text_a += \" \"\n",
    "    text_a += claim.claimant if claim.claimant else \"no claimant\"\n",
    "    text_a += \" \"\n",
    "    text_a += claim.date.split()[0].split(\"T\")[0] if claim.date else \"no date\"\n",
    "    return clean_text(text_a)\n",
    "\n",
    "def create_text_b_content(article):\n",
    "    text_b = \"\"\n",
    "    if article.source:\n",
    "        text_b += article.source + \". \"\n",
    "    if article.title:\n",
    "        text_b += article.title + \". \"\n",
    "    if article.url:\n",
    "        url_words = extract_words_from_url(article.url)\n",
    "        if url_words:\n",
    "            text_b += \" \".join(url_words) + \". \"\n",
    "    if article.content:\n",
    "        text_b += article.content\n",
    "    return clean_text(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYiga7bObTzu"
   },
   "source": [
    "# Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_b_curated(article, claim):\n",
    "    support = []\n",
    "    for sent in article.nlp.sents:\n",
    "        support.append({\n",
    "            \"text\": sent.text,\n",
    "            \"score\": claim.nlp.similarity(sent)\n",
    "        })\n",
    "    support = heapq.nlargest(16, support, key=lambda x: x[\"score\"])\n",
    "    claim.support[article.index] = support\n",
    "    text_b = clean_text(\" \".join([s[\"text\"] for s in support]))\n",
    "    return text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9519db0fbe8b49d7886bb63325c56ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "claims_dict = {}\n",
    "articles_dict = {}\n",
    "\n",
    "examples = []\n",
    "for res in tqdm(responses):\n",
    "    claim = res[\"claim\"]\n",
    "    claim.text_a = create_text_a(claim)\n",
    "    claim.nlp = nlp(claim.text_a, disable=[\"textcat\", \"tagger\", \"parser\", \"ner\"])\n",
    "    claim.res = res\n",
    "    claim.support = {}\n",
    "    \n",
    "    claims_dict[claim.index] = claim\n",
    "\n",
    "    hits = [hit[\"article\"] for hit in res[\"res\"][\"hits\"][\"hits\"]]\n",
    "    related_articles_url_set = set(claim.related_articles.values())\n",
    "\n",
    "    for article in hits[:16]:\n",
    "        if not hasattr(article, \"nlp\"):\n",
    "            article.nlp = nlp(create_text_b_content(article), disable=[\"textcat\", \"tagger\", \"ner\"])\n",
    "        if not hasattr(article, \"text_b\"):\n",
    "            article.text_b = create_text_b_curated(article, claim)\n",
    "        articles_dict[article.index] = article\n",
    "            \n",
    "        examples.append(SequenceClassificationExample(\n",
    "            guid=claim.index,\n",
    "            text_a=claim.text_a,\n",
    "            text_b=article.text_b,\n",
    "            label=1 if article.url in related_articles_url_set else 0,\n",
    "            art_id=article.index\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "15000\n",
      "3839\n"
     ]
    }
   ],
   "source": [
    "print(len(claims_dict))\n",
    "print(len(claims_dict)*30)\n",
    "print(len(articles_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _visit(claim):\n",
    "#     _examples = []\n",
    "#     hits = [(hit[\"score\"], hit[\"article\"]) for hit in res[\"res\"][\"hits\"][\"hits\"]][:16]\n",
    "#     related_articles_url_set = set(claim.related_articles.values())\n",
    "    \n",
    "#     for score, article in hits:\n",
    "#         text_b = create_text_b_curated(article, claim)\n",
    "            \n",
    "#         _examples.append(SequenceClassificationExample(\n",
    "#             guid=claim.index,\n",
    "#             text_a=claim.text_a,\n",
    "#             text_b=text_b,\n",
    "#             label=1 if article.url in related_articles_url_set else 0,\n",
    "#             art_id=article.index\n",
    "#         ))\n",
    "#     return _examples\n",
    "\n",
    "# pool = multiprocessing.Pool(16)\n",
    "# examples = []\n",
    "# misses = 0\n",
    "# for result in tqdm(pool.imap_unordered(_visit, responses), total=len(responses)):\n",
    "#     if result is None:\n",
    "#         misses += 1\n",
    "#     else:\n",
    "#         examples.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DamT2JLogphk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"guid\": \"Phase2ValidationDataset/383\",\n",
       "  \"text_a\": \"\\\"Huge! Results From Breaking Chloroquine Study Show 100% Cure Rate For Patients Infected With The Coronavirus.\\\" Facebook post 2020-03-19\",\n",
       "  \"text_b\": \"school of medicine advisor announced a 100% cure rate in a controlled study done in france of 40 people with the #chinacoronavirus with a malaria drug called #hydroxychloroquine. results from breaking chloroquine study show 100% cure rate for patients infected with the coronavirus | tea party (gateway pundit) \\u2013 on monday dr. anthony fauci, director of the national institute of allergy and infectious diseases, announced that the first trial vaccine for the coronavirus is now being tested. the test includes 45 people age 18-55 and they are receiving two injections, one at zero days, one at 28 days. tea party huge results from breaking chloroquine study show cure rate for patients infected with the coronavirus. dr. rigano said their study found that those covid-19 patients who took hydroxy-chloroquine were found free of the disease in 6 days. after 6 days 100% of patients treated with hcq + azithromycin were virologically cured p-value via the ingraham angle: last night on tucker carlson\\u2019s show on fox news, dr. rigano, discussed the latest findings where the test patients had 100% cure. as of 20 march 2020 dr. fauci stayed that there has been no trial run on this drug. ** an effective treatment for coronavirus (covid-19) laura invited dr. gregory rigano, the co-author of the study to discuss the latest findings. the trial results is still months away. thegatewaypundit.com/2020/03/huge-results-from-breaking-chloroquine-study-show-100-cure-rate-for-patients-infected-with-the-coronavirus/ do not know who wrote this article but they are lying according to dr. fauci as of 11am today mach 20th 2020. in case you do you can get an extra 300 week after week to your account just working from home four hours each day\\u2026 check out this\\u2026 the patients were testing negative for the coronavirus in six days! the individuals will then be followed for one year. on monday night laura ingraham reported that a new study revealed the anti-viral medication chloroquine is successful in fighting the coronavirus. www.work46.com so will this ever be on hollywood/demoncrap news networks ?\",\n",
       "  \"label\": 1,\n",
       "  \"art_id\": \"https://www.teaparty.org/huge-results-from-breaking-chloroquine-study-show-100-cure-rate-for-patients-infected-with-the-coronavirus-432559/\"\n",
       "}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Ohio man died from complications related to the COVID-19 coronavirus disease weeks after he downplayed the virus on social media.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(claims_dict.values())[-10].claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(list(claims_dict.values())[-10].support, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCteL6dXbX53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(len(responses)*16)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ocq58LFblA_"
   },
   "source": [
    "# Predict\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNS6evMZbpsU"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"castorini/monot5-base-msmarco\"\n",
    "# \"castorini/monobert-large-msmarco\"\n",
    "# \"nboost/pt-bert-large-msmarco\"]:\n",
    "pretrained_model_name_or_path = \"castorini/monobert-large-msmarco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 04:01:32,139] INFO:transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/config.json from cache at /home/jay/.cache/torch/transformers/643500d870067d59f219f7b5652919267c01bfa98024e2e74f53b28c1b6aff2b.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "[2020-07-12 04:01:32,140] INFO:transformers.configuration_utils: Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-07-12 04:01:32,140] INFO:transformers.tokenization_utils_base: Model name 'castorini/monobert-large-msmarco' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'castorini/monobert-large-msmarco' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[2020-07-12 04:01:32,878] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/vocab.txt from cache at /home/jay/.cache/torch/transformers/7e579d33b9ca381ae377521c4918fb66ee50a10d0fa955173dc5ad550a05b40c.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-07-12 04:01:32,878] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/added_tokens.json from cache at /home/jay/.cache/torch/transformers/cba5f19d6689880029e5bccfd5e3c905f2067168a27322e8c7a4bf55b5a9a0ad.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "[2020-07-12 04:01:32,878] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/special_tokens_map.json from cache at /home/jay/.cache/torch/transformers/01358c6ecc96532f113f6b9633dcef2fbe2e8142997f6b296df5a9ffb4ae19ab.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "[2020-07-12 04:01:32,879] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/tokenizer_config.json from cache at /home/jay/.cache/torch/transformers/1ed92dc6896c1ff8e5e65d28cb4bebc094ac70f3c961fabb25e0e1e4729b919c.3fc3a7b9028b25d25e6393f1d27ed643eb6ba9ce4113c05aa13277951cff117c\n",
      "[2020-07-12 04:01:32,879] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/tokenizer.json from cache at None\n",
      "[2020-07-12 04:01:33,155] INFO:transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/config.json from cache at /home/jay/.cache/torch/transformers/643500d870067d59f219f7b5652919267c01bfa98024e2e74f53b28c1b6aff2b.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "[2020-07-12 04:01:33,156] INFO:transformers.configuration_utils: Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-07-12 04:01:33,305] INFO:transformers.modeling_utils: loading weights file https://cdn.huggingface.co/castorini/monobert-large-msmarco/pytorch_model.bin from cache at /home/jay/.cache/torch/transformers/c48fb89cc59db72299f77c13a9f914ae2dd73cae21dcdcef4281c0308e2f4075.454581226736f9af4828c8d3a034ecd8c0c174c620fcb9356260b0342591de76\n",
      "[2020-07-12 04:01:42,118] INFO:transformers.modeling_utils: All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[2020-07-12 04:01:42,118] INFO:transformers.modeling_utils: All the weights of BertForSequenceClassification were initialized from the model checkpoint at castorini/monobert-large-msmarco.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = SequenceClassificationModel.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 04:01:42,715] INFO:valerie.modeling: ... converting examples to features ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4011fd8f559847deb87db59619ce7259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='converting examples to features', max=8000.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "examples_dataset = model.create_dataset(examples, nproc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 04:02:00,398] INFO:transformers.training_args: PyTorch: setting up devices\n",
      "[2020-07-12 04:02:04,803] INFO:transformers.trainer: Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[2020-07-12 04:02:04,806] INFO:transformers.trainer: ***** Running Prediction *****\n",
      "[2020-07-12 04:02:04,806] INFO:transformers.trainer:   Num examples = 8000\n",
      "[2020-07-12 04:02:04,807] INFO:transformers.trainer:   Batch size = 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a6a29f831e425ca245f9c50d91568b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Prediction', max=4.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/miniconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_output = model.predict(examples_dataset, predict_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2YvlHrobmdq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b5d3bae0464f1080c465a64217e9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "api\n",
      "{\n",
      "  \"perfect_rerank_score\": 1.1557801493610302,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.6617083840074162,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "trans\n",
      "{\n",
      "  \"perfect_rerank_score\": 1.0491232931485748,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.7396992039496483,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "both\n",
      "{\n",
      "  \"perfect_rerank_score\": 1.0491232931485748,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.7086715926336704,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "claims_dict = {res[\"claim\"].index: res[\"claim\"] for res in responses if res[\"res\"]}\n",
    "api_scores_dict = {\n",
    "    res[\"claim\"].index: {\n",
    "        hit[\"article\"].index: hit[\"score\"] for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    }\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "\n",
    "rerank_just_api_responses = {\n",
    "    res[\"claim\"].index: [\n",
    "        (hit[\"article\"].index, hit[\"score\"]) for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    ]\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "rerank_just_trans_responses = {res[\"claim\"].index: [] for res in responses if res[\"res\"]}\n",
    "\n",
    "rerank_both_responses = {\n",
    "    res[\"claim\"].index: []\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "for example, proba in tqdm(zip(examples, predict_output.predictions)):\n",
    "    proba = float(proba[1]) # get probability that the article is related\n",
    "\n",
    "    rerank_just_trans_responses[example.guid].append((example.art_id, proba))\n",
    "    rerank_both_responses[example.guid].append((example.art_id, proba + api_scores_dict[example.guid][example.art_id]))\n",
    "    \n",
    "print('api')\n",
    "print(json.dumps(compute_responses_score(rerank_just_api_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('trans')\n",
    "print(json.dumps(compute_responses_score(rerank_just_trans_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('both')\n",
    "print(json.dumps(compute_responses_score(rerank_both_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6Gu45bVn1sy"
   },
   "source": [
    "# Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NV2g5ha1n455"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 1.1557801493610302,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.6617083840074162,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_just_api_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vp6mnXWZn5zW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 1.0491232931485748,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.7396992039496483,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_just_trans_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0W5F38On61c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 1.0491232931485748,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.7086715926336704,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_both_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "__experiment.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
