{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFcE0lSqYag4"
   },
   "source": [
    "# Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruKsHPGPblRV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import heapq\n",
    "import pickle\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from valerie.utils import get_logger\n",
    "from valerie.preprocessing import extract_words_from_url, clean_text\n",
    "from valerie.scoring import validate_predictions_phase2, compute_score_phase2\n",
    "from valerie.modeling import SequenceClassificationModel, SequenceClassificationDataset, SequenceClassificationExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sT0p85DUbe5L"
   },
   "outputs": [],
   "source": [
    "_logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTPKlXP4dElX"
   },
   "outputs": [],
   "source": [
    "with open(\"data/phase2-3/processed/responses.pkl\", \"rb\") as fi:\n",
    "    responses = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = responses[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1E-o-DMbBsR"
   },
   "outputs": [],
   "source": [
    "def compute_responses_score(results, claims_dict):\n",
    "    predictions = {}\n",
    "    perfect_predictions = {}\n",
    "    labels = {}\n",
    "\n",
    "    for k, hits in results.items():\n",
    "        claim = claims_dict[k]\n",
    "        labels[claim.id] = claim.to_dict()\n",
    "        \n",
    "        hits = sorted(hits, key=lambda x: x[1], reverse=True) # sort by score\n",
    "        predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits[:2]])\n",
    "            }\n",
    "        }\n",
    "        perfect_predictions[claim.id] = {\n",
    "            \"label\": claim.label,\n",
    "            \"explanation\": \"\",\n",
    "            \"related_articles\": {\n",
    "                i + 1: x\n",
    "                for i, x in enumerate([v[0] for v in hits if v[0] in claim.related_articles.values()][:2])\n",
    "            }\n",
    "        }\n",
    "\n",
    "    validate_predictions_phase2(predictions)\n",
    "    score = compute_score_phase2(labels, predictions)\n",
    "    validate_predictions_phase2(perfect_predictions)\n",
    "    perfect_score = compute_score_phase2(labels, perfect_predictions)\n",
    "    return {\n",
    "        \"perfect_rerank_score\": perfect_score[\"score\"],\n",
    "        \"perfect_rerank_error\": perfect_score[\"error\"],\n",
    "        \"api_score\": score[\"score\"],\n",
    "        \"api_error\": score[\"error\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_a(claim):\n",
    "    text_a = claim.claim\n",
    "    text_a += \" \"\n",
    "    text_a += claim.claimant if claim.claimant else \"no claimant\"\n",
    "    text_a += \" \"\n",
    "    text_a += claim.date.split()[0].split(\"T\")[0] if claim.date else \"no date\"\n",
    "    return clean_text(text_a)\n",
    "\n",
    "def create_text_b_content(article):\n",
    "    text_b = \"\"\n",
    "    if article.source:\n",
    "        text_b += article.source + \". \"\n",
    "    if article.title:\n",
    "        text_b += article.title + \". \"\n",
    "    if article.url:\n",
    "        url_words = extract_words_from_url(article.url)\n",
    "        if url_words:\n",
    "            text_b += \" \".join(url_words) + \". \"\n",
    "    if article.content:\n",
    "        text_b += article.content\n",
    "    return clean_text(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Spacy on Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7fbefdd5694bc1a5ac4f27fa214692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "misses = 0\n",
    "claims_list = []\n",
    "for res in tqdm(responses):\n",
    "    if not res[\"res\"]:\n",
    "        misses += 1\n",
    "        continue\n",
    "    claim = res[\"claim\"]\n",
    "    claim.text_a = create_text_a(claim)\n",
    "    claim.res = res\n",
    "    claim.support = {}\n",
    "    claims_list.append(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_texts = [claim.text_a for claim in claims_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326bd230aff943bb87d1fd74c270c397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "claims_docs = [doc for doc in tqdm(nlp.pipe(claims_texts, n_process=16, disable=[\"textcat\", \"tagger\", \"parser\", \"ner\"]), total=len(claims_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc041d26b5c4c4f9df2bd69b8452f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "claims_dict = {}\n",
    "for claim, doc in tqdm(zip(claims_list, claims_docs)):\n",
    "    claim.doc = doc\n",
    "    claims_dict[claim.index] = claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claims_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(claims_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362ffd229d9b40ddb0e752350069f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "misses = 0\n",
    "articles_list = []\n",
    "for res in tqdm(responses):\n",
    "    if not res[\"res\"]:\n",
    "        misses += 1\n",
    "        continue\n",
    "    for hit in res[\"res\"][\"hits\"][\"hits\"]:\n",
    "        article = hit[\"article\"]\n",
    "#         article.text_b = create_text_b_content(article)\n",
    "        articles_list.append(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26629"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19626"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = list(set(articles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71c3ca9d6bb429b98be4ba7f6d9367f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def _text_b_text(article):\n",
    "    return article, create_text_b_content(article)\n",
    "\n",
    "articles_texts = {}\n",
    "pool = multiprocessing.Pool(16)\n",
    "for article, text_b in tqdm(pool.imap_unordered(_text_b_text, articles_list), total=len(articles_list)):\n",
    "    articles_texts[article.index] = text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles_list:\n",
    "    article.text_b = articles_texts[article.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a134fdb2849d42f18797a359610e067a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles_texts = [article.text_b for article in tqdm(articles_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d26d31fd1934bbcbf97d6632de4b846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19626.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles_docs = [doc for doc in tqdm(nlp.pipe(articles_texts, n_process=16, disable=[\"textcat\", \"tagger\", \"ner\"]), total=len(articles_texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac6a4de4714bbbaeafe5b40016b732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "articles_dict = {}\n",
    "for article, doc in tqdm(zip(articles_list, articles_docs)):\n",
    "    article.doc = doc\n",
    "    articles_dict[article.index] = article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYiga7bObTzu"
   },
   "source": [
    "# Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_b_curated(article, claim):\n",
    "    support = []\n",
    "    for sent in article.doc.sents:\n",
    "        support.append({\n",
    "            \"text\": sent.text,\n",
    "            \"score\": claim.doc.similarity(sent)\n",
    "        })\n",
    "    support = heapq.nlargest(16, support, key=lambda x: x[\"score\"])\n",
    "    claim.support[article.index] = support\n",
    "    text_b = clean_text(\" \".join([s[\"text\"] for s in support]))\n",
    "    return text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<02:17,  7.26it/s]/home/jay/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \n",
      "100%|██████████| 1000/1000 [02:42<00:00,  6.15it/s]\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for claim in tqdm(claims_dict.values()):\n",
    "    hits_indices = [hit[\"url\"] for hit in claim.res[\"res\"][\"hits\"][\"hits\"]]\n",
    "    hits = [articles_dict[idx] for idx in hits_indices]\n",
    "    \n",
    "    related_articles_url_set = set(claim.related_articles.values())\n",
    "\n",
    "    for article in hits[:16]:\n",
    "        article.text_b = create_text_b_curated(article, claim)\n",
    "\n",
    "        examples.append(SequenceClassificationExample(\n",
    "            guid=claim.index,\n",
    "            text_a=claim.text_a,\n",
    "            text_b=article.text_b,\n",
    "            label=1 if article.url in related_articles_url_set else 0,\n",
    "            art_id=article.index\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "19626\n",
      "\n",
      "16000\n",
      "15975\n"
     ]
    }
   ],
   "source": [
    "print(len(claims_dict))\n",
    "print(len(articles_dict))\n",
    "print()\n",
    "print(len(claims_dict)*16)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DamT2JLogphk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"guid\": \"Phase2Dataset/6093\",\n",
       "  \"text_a\": \"\\\"To deprive\\\" underprivileged children \\\"of a meat-based protein during school lunch is most likely depriving them of their only source of protein for the day.\\\" Todd Staples 2014-09-08\",\n",
       "  \"text_b\": \"For as little as $6 a month, you can help support our independent journalism \\u2014 and enjoy special benefits and status as a Times of Israel Community member! But we would like to invite readers who can afford to do so, and for whom The Times of Israel has become important, to help support our journalism by joining Join now and for as little as $6 a month you can both help ensure our ongoing investment in quality journalism, and enjoy special status and benefits as a Times of Israel Community member. The protein is known to have restorative qualities, but the problem is how to get it to the brain, she said. Thus, the neural growth factor proteins, if given in drug form, do not pass through the barrier, and even if they could, they would not live long enough to make the long journey to the brain, Segal said. With the use of the chips as a vehicle, the protein no longer needs to cross the blood-brain barrier, since the chip is inserted directly into the brain. We've achieved this by investing ever-greater resources in our journalism while keeping all of the content on our site free. It primarily strikes the elderly, and among those age 85 and up it reaches a prevalence of some 30%. Determined to keep readers fully informed and enable them to form and flesh out their own opinions, The Times of Israel has gradually established itself as the leading source of independent and fair-minded journalism on Israel, the region and the Jewish world. The researchers aim to eventually be able to load the chip with medication that releases even more gradually, over the period of a year, to prolong the drug\\u2019s effect. Through precise control of various features, including the dimension of the chips\\u2019 pores and the chemical properties of their surface, the researchers were able to create a silicone structure that retains the protein in its active form and then releases it gradually, over a period of about a month, to the target area in the brain. This protective barrier, however, also restricts the passage of drugs from the bloodstream to the brain, making it difficult for brain-curing medications to get through. The team is already conducting pre-clinical studies on animals, at Bar-Ilan , Segal explained, and hopes to expand them to clinical trials if all goes as hoped. Due to the increase in life expectancy, the overall incidence of the disease has grown and it is today referred to as the epidemic of the 21st century. Delivering the protein to the target area is not a simple task because the brain is shielded by the blood-brain barrier from infiltration by bacteria and harmful substances in the blood. As part of the study, Prof. Shefi of Bar-Ilan reworked the gun, transforming it into \\u201csomething like a nose spray\\u201d that injects the silicon chip with the protein particles into the brain via the nose, bypassing the blood-brain barrier. The second way to get the proteins to the brain is by using a \\u201cgene gun\\u201d \\u2014 a device that has been developed to inject DNA into plant cells in order to transform their genetic structure.\",\n",
       "  \"label\": null,\n",
       "  \"art_id\": \"https://www.timesofisrael.com/silicon-chip-becomes-vehicle-to-deliver-alzheimers-busting-protein-to-brain/\"\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"U.S. troops are getting only 4 hours of Ebola training before deployment to Liberia.\"'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(claims_dict.values())[-1].claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(list(claims_dict.values())[-1].support, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCteL6dXbX53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "15975\n"
     ]
    }
   ],
   "source": [
    "print(len(responses)*16)\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Ocq58LFblA_"
   },
   "source": [
    "# Predict\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNS6evMZbpsU"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"dryrun\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"castorini/monot5-base-msmarco\"\n",
    "# \"castorini/monobert-large-msmarco\"\n",
    "# \"nboost/pt-bert-large-msmarco\"]:\n",
    "pretrained_model_name_or_path = \"castorini/monobert-large-msmarco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 14:48:34,942] INFO:transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/config.json from cache at /home/jay/.cache/torch/transformers/643500d870067d59f219f7b5652919267c01bfa98024e2e74f53b28c1b6aff2b.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "[2020-07-12 14:48:34,944] INFO:transformers.configuration_utils: Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-07-12 14:48:34,945] INFO:transformers.tokenization_utils_base: Model name 'castorini/monobert-large-msmarco' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming 'castorini/monobert-large-msmarco' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[2020-07-12 14:48:35,709] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/vocab.txt from cache at /home/jay/.cache/torch/transformers/7e579d33b9ca381ae377521c4918fb66ee50a10d0fa955173dc5ad550a05b40c.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "[2020-07-12 14:48:35,709] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/added_tokens.json from cache at /home/jay/.cache/torch/transformers/cba5f19d6689880029e5bccfd5e3c905f2067168a27322e8c7a4bf55b5a9a0ad.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "[2020-07-12 14:48:35,709] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/special_tokens_map.json from cache at /home/jay/.cache/torch/transformers/01358c6ecc96532f113f6b9633dcef2fbe2e8142997f6b296df5a9ffb4ae19ab.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "[2020-07-12 14:48:35,709] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/tokenizer_config.json from cache at /home/jay/.cache/torch/transformers/1ed92dc6896c1ff8e5e65d28cb4bebc094ac70f3c961fabb25e0e1e4729b919c.3fc3a7b9028b25d25e6393f1d27ed643eb6ba9ce4113c05aa13277951cff117c\n",
      "[2020-07-12 14:48:35,709] INFO:transformers.tokenization_utils_base: loading file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/tokenizer.json from cache at None\n",
      "[2020-07-12 14:48:35,899] INFO:transformers.configuration_utils: loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/castorini/monobert-large-msmarco/config.json from cache at /home/jay/.cache/torch/transformers/643500d870067d59f219f7b5652919267c01bfa98024e2e74f53b28c1b6aff2b.4c88e2dec8f8b017f319f6db2b157fee632c0860d9422e4851bd0d6999f9ce38\n",
      "[2020-07-12 14:48:35,900] INFO:transformers.configuration_utils: Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[2020-07-12 14:48:36,079] INFO:transformers.modeling_utils: loading weights file https://cdn.huggingface.co/castorini/monobert-large-msmarco/pytorch_model.bin from cache at /home/jay/.cache/torch/transformers/c48fb89cc59db72299f77c13a9f914ae2dd73cae21dcdcef4281c0308e2f4075.454581226736f9af4828c8d3a034ecd8c0c174c620fcb9356260b0342591de76\n",
      "[2020-07-12 14:48:45,005] INFO:transformers.modeling_utils: All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[2020-07-12 14:48:45,006] INFO:transformers.modeling_utils: All the weights of BertForSequenceClassification were initialized from the model checkpoint at castorini/monobert-large-msmarco.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = SequenceClassificationModel.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 14:48:55,655] INFO:valerie.modeling: ... converting examples to features ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e214008b2224168ba25aa4d99fa8e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='converting examples to features', max=15975.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "examples_dataset = model.create_dataset(examples, nproc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3,4,5,6,7\"\n",
    "!echo $CUDA_VISIBLE_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-07-12 14:49:12,517] INFO:transformers.training_args: PyTorch: setting up devices\n",
      "[2020-07-12 14:49:16,210] INFO:transformers.trainer: Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "[2020-07-12 14:49:16,217] INFO:transformers.trainer: ***** Running Prediction *****\n",
      "[2020-07-12 14:49:16,218] INFO:transformers.trainer:   Num examples = 15975\n",
      "[2020-07-12 14:49:16,220] INFO:transformers.trainer:   Batch size = 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972030be90964d4d96efb27f124f0c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Prediction', max=8.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict_output = model.predict(examples_dataset, predict_batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2YvlHrobmdq"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15975it [00:00, 381521.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.8630615319780865,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.448855518144285,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "trans\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.7876843776993298,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.5356888355106825,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "both\n",
      "{\n",
      "  \"perfect_rerank_score\": 0.7876843776993298,\n",
      "  \"perfect_rerank_error\": \"'None'\",\n",
      "  \"api_score\": 0.4956699996625343,\n",
      "  \"api_error\": \"'None'\"\n",
      "}\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "claims_dict = {res[\"claim\"].index: res[\"claim\"] for res in responses if res[\"res\"]}\n",
    "api_scores_dict = {\n",
    "    res[\"claim\"].index: {\n",
    "        hit[\"article\"].index: hit[\"score\"] for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    }\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "\n",
    "rerank_just_api_responses = {\n",
    "    res[\"claim\"].index: [\n",
    "        (hit[\"article\"].index, hit[\"score\"]) for hit in res[\"res\"][\"hits\"][\"hits\"]\n",
    "    ]\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "rerank_just_trans_responses = {res[\"claim\"].index: [] for res in responses if res[\"res\"]}\n",
    "\n",
    "rerank_both_responses = {\n",
    "    res[\"claim\"].index: []\n",
    "    for res in responses\n",
    "    if res[\"res\"]\n",
    "}\n",
    "\n",
    "for example, proba in tqdm(zip(examples, predict_output.predictions)):\n",
    "    proba = float(proba[1]) # get probability that the article is related\n",
    "\n",
    "    rerank_just_trans_responses[example.guid].append((example.art_id, proba))\n",
    "    rerank_both_responses[example.guid].append((example.art_id, proba + api_scores_dict[example.guid][example.art_id]))\n",
    "    \n",
    "print('api')\n",
    "print(json.dumps(compute_responses_score(rerank_just_api_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('trans')\n",
    "print(json.dumps(compute_responses_score(rerank_just_trans_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print('both')\n",
    "print(json.dumps(compute_responses_score(rerank_both_responses, claims_dict), indent=2))\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6Gu45bVn1sy"
   },
   "source": [
    "# Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NV2g5ha1n455"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 0.8630615319780865,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.448855518144285,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_just_api_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vp6mnXWZn5zW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 0.7876843776993298,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.5356888355106825,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_just_trans_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0W5F38On61c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perfect_rerank_score': 0.7876843776993298,\n",
       " 'perfect_rerank_error': \"'None'\",\n",
       " 'api_score': 0.4956699996625343,\n",
       " 'api_error': \"'None'\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_responses_score(rerank_both_responses, claims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "__experiment.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
